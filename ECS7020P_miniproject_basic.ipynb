{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bO2x01g__wC"
   },
   "source": [
    "# 1 Author:\n",
    "\n",
    "Student Name: Andrew Porter\n",
    "Student ID: 220999027",
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbTPeg-H__zX"
   },
   "source": [
    "# 2 Problem formulation\n",
    "\n",
    "The objective of this machine learning pipeline is to take as input a photo of a dish that contains either rice or chips and predict whether the picture has rice or chips. This binary classification problem is interesting as it involves image processing, and the creation of a diverse ensemble model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2sSMMwrA__2Q"
   },
   "source": [
    "# 3 Machine Learning Pipeline\n",
    "\n",
    "Dataset Loading and Preprocessing:\n",
    "- The dataset is loaded from the 'MLEnd Yummy Dataset,' and a new column 'Rice_Chips' is created based on the 'Ingredients' column.\n",
    "- Rows without rice or chips are removed, and label encoding is applied to create a new column 'Y_encoded.'\n",
    "- Undesired entries using the word 'chips' are removed.\n",
    "- Note: there is a severe imbalance between rice and chips samples, this is dealt with during the transformation stage.\n",
    "- Index values are converted from file name to image paths and are loaded into initial X variables for training and test sets.\n",
    "- Each desired image is loaded and processed to ensure they are square and resized to a consistent size (200x200 pixels).\n",
    "\n",
    "Feature Extraction and Transformation:\n",
    "- Yellow component extraction and Gray-Level Co-occurrence Matrix (GLCM) features are computed for each image.\n",
    "- Standardization is applied to scale these features for uniformity (using StandardScaler).\n",
    "- A combination of undersampling and oversampling is utilised to correct the data imbalance.\n",
    "\n",
    "Base Models and Ensemble:\n",
    "- We construct an ensemble, composed of three diverse base models.\n",
    "- Three base models are constructed: Support Vector Machine (SVM), Random Forest, and Gradient Boosting.\n",
    "- A voting classifier is used to combine the diverse classification capabilities of the models to enhance overall classifier performance.\n",
    "- Cross-validation is used to tune hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDdai9GuAjJF"
   },
   "source": [
    "# 4 Transformation Stage\n",
    "\n",
    "We extract three features, the yellow component and Gray-Level Co-occurrence Matrix (GLCM). Both of which are important in discerning the presence of certain ingredients, such as rice or chips, in the images. The yellow component extraction focuses on the color characteristics associated with specific food items, which often exhibit distinct yellow tones (more relevant in this case). On the other hand, the GLCM features capture the spatial relationships and textures present in the images, providing valuable information about the arrangement of pixel intensities and discerning subtle textural differences between images containing rice and chips. Together, these features contribute to a holistic representation of the visual attributes associated with rice and chips, enabling the model to discriminate between these food items based on a diverse set of visual cues. The created functions take in the resized 200 by 200 pixel images from our training and test data, conduct the transformation, and load these features into new variables for these sets. These 3 features are then standardised to improve functionality across all 3 ensemble models.\n",
    "\n",
    "We also combine undersampling and oversampling to correct the class imbalance. The utilisation of both techniques is to strike a balance between the loss of rice data (undersampling majority) and the risk of overfitting via oversampling (replicated samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vj76VgyFApq2"
   },
   "source": [
    "# 5 Modelling\n",
    "\n",
    "In this task of classifying images into rice or chips, I've chosen a Voting Classifier with three diverse base models: RandomForestClassifier, Support Vector Classifier (SVC), and GradientBoostingClassifier.\n",
    "\n",
    "RandomForestClassifier : An ensemble method that builds multiple decision trees individually during training and merges them together to get a more accurate and stable prediction.\n",
    "\n",
    "Support Vector Classifier: A powerful classifier that works well in high-dimensional spaces, making it suitable for image classification. The 'linear' kernel is chosen for simplicity and interpretability.\n",
    "\n",
    "GradientBoostingClassifier: Another ensemble method which also builds trees, however, each corrects the errors of the previous one, placing more emphasis on misclassification and correction. It often provides high accuracy and generalization.\n",
    "\n",
    "These three models were chosen due to the diversity in their approach, collectively producing an enhanced classification system.\n",
    "The Voting Classifier combines these models using soft voting, considering the probability estimates for classification. This approach takes into account the confidence of each model in its predictions.\n",
    "\n",
    "Various models were trialled manually (including kNN, a logistic regression classifier, AdaBoosting), however, this current combination of models produced the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVa4N2gZAsCg"
   },
   "source": [
    "# 6 Methodology\n",
    "\n",
    "To train and validate the models, a robust methodology is applied. The following steps are involved:\n",
    "\n",
    "Data Preprocessing: Images are loaded, resized, and standardized. Features are extracted, and the dataset is split into training and testing sets (using dataframe 'Benchmark_A' values).\n",
    "Class Imbalance Correction: A combination of oversampling and undersampling is applied to address class imbalances (283 class each in training set). This enhances the model's ability to learn patterns from the minority class (chips) without too much data loss from majority.\n",
    "Cross-Validation for Hyperparameter Tuning: The ensemble model is constructed and iteratively trained with an incorporated cross-validation component, evaluating the hyperparameter combinations (using F1 score).\n",
    "Tuned Ensemble Model Training: The ensemble model is then comprehensively trained on the whole training set, utilising the tuned cross-validated hyperparameters.\n",
    "Performance Evaluation: The model's performance is, again, assessed using the F1 score metric. This is due to the initial imbalanced nature of the dataset. Additionally, a confusion matrix is utilized to understand how well the model classifies instances into true positives, true negatives, false positives, and false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Sn1OQ3jAsGd"
   },
   "source": [
    "# 7 Dataset\n",
    "\n",
    "The dataset employed in this study originates from the 'MLEnd Yummy Dataset'. This dataset encompasses various culinary dishes submitted by Queen Mary University London students. We modify the index column which initially contains file names for each photo to contain the path. The indices of the dataframe are then separated into training and test entries. We then load them into our resizing function and afterwards, begin to extract our features. These features are stored in an array (see below). [Note: I had previously manually split the fully transformed training data into separate training and validation sets, saving them locally (to prevent the need to continously resize and transform the data open re-opening). However, I later chose to use k-folds cross validation instead, therefore I reload these datapoints and rejoin them into one training set below]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GGb5BoBZUdsm",
    "outputId": "9832b50f-94df-4efe-a515-ad3129c7abbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.94061338, -0.87937572,  0.843526  ],\n",
       "       [-0.06553251, -1.91337857,  1.19904594],\n",
       "       [ 0.87997125,  0.96878868, -0.54841359],\n",
       "       ...,\n",
       "       [-0.77680363,  0.92273773, -0.82945811],\n",
       "       [ 1.09277083, -0.77280363,  0.09609824],\n",
       "       [ 0.07684419, -0.24612981,  0.61212479]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from skimage import exposure\n",
    "from skimage.color import rgb2hsv, rgb2gray\n",
    "import skimage as ski\n",
    "\n",
    "import os, sys, re, pickle, glob\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "import IPython.display as ipd\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# Loads variables from file\n",
    "X_train_scaled = np.load('/content/gdrive/MyDrive/X_train_scaled.npy')\n",
    "\n",
    "X_test_scaled = np.load('/content/gdrive/MyDrive/X_test_scaled.npy')\n",
    "\n",
    "X_val_scaled = np.load('/content/gdrive/MyDrive/X_val_scaled.npy')\n",
    "\n",
    "Y_val = np.load('/content/gdrive/MyDrive/Y_val.npy')\n",
    "Y_train = np.load('/content/gdrive/MyDrive/Y_train.npy')\n",
    "Y_test = np.load('/content/gdrive/MyDrive/Y_test.npy')\n",
    "\n",
    "# Rejoins data\n",
    "\n",
    "X_train_scaled = np.vstack((X_train_scaled, X_val_scaled))\n",
    "Y_train = np.concatenate((Y_train, Y_val), axis=0)\n",
    "\n",
    "\n",
    "\n",
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r9nMmmvnbpgH",
    "outputId": "0c44d145-f29e-4557-a97b-996db0dfde08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 283, 1: 283}) (566, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Oversampling and undersampling\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# defines oversampling strategy\n",
    "over = RandomOverSampler(sampling_strategy=0.5) # Increases minority samples to half majority through random replication\n",
    "# fit and apply the transform\n",
    "X_train_scaled, Y_train = over.fit_resample(X_train_scaled, Y_train)\n",
    "\n",
    "under = RandomUnderSampler(sampling_strategy='auto') # Reduces majority samples to same size as minority\n",
    "# fit and apply the transform\n",
    "X_train_scaled, Y_train= under.fit_resample(X_train_scaled, Y_train)\n",
    "# summarizes class distribution\n",
    "print(Counter(Y_train), X_train_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77D4Pi5pAsJ_"
   },
   "source": [
    "# 8 Results\n",
    "\n",
    "Our first iteration of the model involves the construction and training of the model with an incorporated hyper-parameter tuning cross-validation stage (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w_9UzUxNwWOH",
    "outputId": "d454bce0-0823-407a-e63c-b995a71e236b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 F1 Score: 0.9026019417475728\n",
      "Fold 2 F1 Score: 0.8935574229691876\n",
      "Fold 3 F1 Score: 0.9026019417475728\n",
      "Fold 4 F1 Score: 0.8939205955334988\n",
      "Fold 5 F1 Score: 0.9205574912891986\n",
      "Best Hyperparameters: {'GB__n_estimators': 113, 'RF__n_estimators': 70, 'SVC__C': 0.1}\n",
      "Test F1 Score with Best Hyperparameters: 0.8385402705213344\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from scipy.stats import randint\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define individual models\n",
    "model1 = RandomForestClassifier()\n",
    "model2 = SVC(probability=True)\n",
    "model3 = GradientBoostingClassifier()\n",
    "\n",
    "# Create the ensemble model\n",
    "ensemble_model = VotingClassifier(estimators=[\n",
    "    ('RF', model1),\n",
    "    ('SVC', model2),\n",
    "    ('GB', model3)\n",
    "], voting='soft')\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_dist = {\n",
    "    'RF__n_estimators': randint(50, 200),\n",
    "    'SVC__C': [0.1, 1, 10],\n",
    "    'GB__n_estimators': randint(50, 150)\n",
    "    # Add other hyperparameters for tuning\n",
    "}\n",
    "\n",
    "# Sets up StratifiedKFold for cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "# Sets up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=ensemble_model,\n",
    "    param_distributions=param_dist,\n",
    "    scoring=scorer,\n",
    "    cv=cv,\n",
    "    n_iter=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fits the RandomizedSearchCV on the training data\n",
    "random_search.fit(X_train_scaled, Y_train)\n",
    "\n",
    "cv_results = random_search.cv_results_\n",
    "for fold, f1_score_fold in zip(range(1, cv.n_splits + 1), cv_results['split0_test_score']):\n",
    "    print(f'Fold {fold} F1 Score: {f1_score_fold}')\n",
    "\n",
    "# Gets the best parameters\n",
    "best_params = random_search.best_params_\n",
    "print(f'Best Hyperparameters: {best_params}')\n",
    "\n",
    "# Evaluates performance on the test set with the best model\n",
    "best_model = random_search.best_estimator_\n",
    "ensemble_f1_test = f1_score(Y_test, best_model.predict(X_test_scaled), average='weighted')\n",
    "print(f'Test F1 Score with Best Hyperparameters: {ensemble_f1_test}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qibsgRuFxNJY"
   },
   "source": [
    "Something to note is the consistency in performance across different folds in cross-validation. This is an important indication of the model's stability and ability to generalize well to different subsets of the training data. If the performance metric such as F1 score remains similar across folds, it suggests that the model is not overly sensitive to variations in the training data and is not overfitting to any particular subset. This is a positive sign, the model is seemingly learning patterns that are likely to generalize well to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8nA0EXesjZV"
   },
   "source": [
    "We now re-train the ensemble model on the whole training set utilising the tuned hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OrW3py4Hd-8Y",
    "outputId": "1682493d-2fc7-4f2f-ff04-fbc3993b214f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8325991189427313\n",
      "Test F1 Score: 0.9068627450980392\n",
      "Confusion Matrix (Test Set):\n",
      "[[  4  17]\n",
      " [ 21 185]]\n"
     ]
    }
   ],
   "source": [
    "# Re-creates the ensemble model with the best hyperparameters\n",
    "best_model = VotingClassifier(estimators=[\n",
    "    ('RF', RandomForestClassifier(n_estimators=best_params['RF__n_estimators'])),\n",
    "    ('SVC', SVC(kernel='linear', C=best_params['SVC__C'], probability=True)),\n",
    "    ('GB', GradientBoostingClassifier(n_estimators=best_params['GB__n_estimators'], random_state=42))\n",
    "], voting='soft')\n",
    "\n",
    "# Trains the model on the entire training dataset\n",
    "best_model.fit(X_train_scaled, Y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "# Makes predictions on the test set\n",
    "ensemble_predictions_test = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate performance\n",
    "test_accuracy = accuracy_score(Y_test, ensemble_predictions_test)\n",
    "test_f1_score = f1_score(Y_test, ensemble_predictions_test)\n",
    "conf_matrix_test = confusion_matrix(Y_test, ensemble_predictions_test)\n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "print(f'Test F1 Score: {test_f1_score}')\n",
    "print('Confusion Matrix (Test Set):')\n",
    "print(conf_matrix_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqC9ieTwtweq"
   },
   "source": [
    "The model seems to be performing well with a high accuracy and F1 score. However, you can see that photos with chips are still being heavily misclassified despite the sample modification attempts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlBV5GVDAsNG"
   },
   "source": [
    "# 9 Conclusions\n",
    "\n",
    "The results of the current form of the ensemble model are positive, but there is great potential for future modifications. To eliminate any classification bias, a more balanced dataset is required. If that is not feasible, a revised sampling section could be implemented. For instance, cross-validation could be employed to determine the optimal combination of oversampling and undersampling. Additionally, varied feature engineering could potentially address the model's misclassification of chips.\n",
    "\n",
    "Overall, this model has established a solid foundation for future research. Experimentation with alternative ensembles of increased complexity and improved engineering would be welcomed. An interesting avenue is the utilisation of stacked ensembles. It would be intriguing to investigate neural networks as a base or a meta-model (MLP) in such a model."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
